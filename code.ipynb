{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})\n",
    "sns.set(rc={'ytick.labelcolor':'black','xtick.labelcolor':'black'}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Functions <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_errors (X, y, trees_grid, model):\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    list_nb_trees = trees_grid\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    for trees in list_nb_trees:\n",
    "        model = model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        train_results.append(mean_absolute_error(y_train, model.predict(X_train)))\n",
    "        test_results.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "    \n",
    "    plot_df = pd.DataFrame({'trees' : trees_grid, 'train_absolute_error': train_results, 'test_absolute_error': test_results})\n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_gridsearch (model, param_grid, X, y, scoring):\n",
    "\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring=scoring, verbose=2, return_train_score=True)\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    return results_df, grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search_results(results_df, x_axis_column, hue):\n",
    "\n",
    "    for i, col in enumerate(results_df.columns):\n",
    "        if (results_df[col].dtype == 'int' or results_df[col].dtype == 'float64') and col not in [x_axis_column, hue, 'rank_test_score']:\n",
    "            plt.figure(i)\n",
    "            sns.lineplot(data=results_df, y=col, x=x_axis_column, hue=hue, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> EDA <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print('Unique values in column %s' %col,'\\n', df[col].unique(),\n",
    "        '\\nNumber of unique values in column %s :' %col, df[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Doing one-hot encoding <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_sex = pd.get_dummies(df['sex'], prefix='sex')\n",
    "dummies_smoker = pd.get_dummies(df['smoker'], prefix='smoker')\n",
    "dummies_region = pd.get_dummies(df['region'], prefix='region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sex', 'smoker', 'region']\n",
    "dfs = [df, dummies_sex, dummies_smoker, dummies_region]\n",
    "df_with_dummies = pd.concat(dfs, axis = 1)\n",
    "df_with_dummies = df_with_dummies.drop(columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Visualizations <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(df.columns):\n",
    "    plt.figure(i)\n",
    "    sns.histplot(x = col, data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(df.columns):\n",
    "    if df[col].dtype == 'int64' or df[col].dtype == 'float64':\n",
    "        plt.figure(i)\n",
    "        sns.boxplot(x = col, data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(df.columns):\n",
    "    if df[col].dtype == 'int64' or df[col].dtype == 'float64':\n",
    "        plt.figure(i)\n",
    "        sns.violinplot(x = col, data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_by_age = df_with_dummies.groupby('age').agg(\n",
    "    median_charges = ('charges', np.median), \n",
    "    mean_bmi = ('bmi', np.mean)\n",
    "    ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_of_smokers_age = (df_with_dummies.groupby('age')['smoker_yes'].sum() / (df_with_dummies.groupby('age')['smoker_no'].sum() + df_with_dummies.groupby('age')['smoker_yes'].sum())).reset_index()\n",
    "percent_of_smokers_age = percent_of_smokers_age.drop(columns = 'age')\n",
    "information_by_age['percent_of_smokers'] = round(percent_of_smokers_age, 2) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_by_age.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'age', y = 'median_charges', data = information_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have one age group which stands out in case of median of charges. On the plot below we can see that this age group also has the largest proportion of smokers\n",
    "information_by_age[information_by_age['median_charges'] == information_by_age['median_charges'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'age', y = 'mean_bmi', data = information_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'age', y = 'percent_of_smokers', data = information_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_by_region = df.groupby('region').agg(\n",
    "    median_charges = ('charges', np.median), \n",
    "    mean_bmi = ('bmi', np.mean),\n",
    "    mean_children = ('children', np.mean)\n",
    "    ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = 'region', y = 'median_charges', data = information_by_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = 'region', y = 'mean_bmi', data = information_by_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = 'region', y = 'mean_children', data = information_by_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('region')['region'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['sex', 'smoker'])['age'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['region', 'smoker'])['age'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['sex']).agg(\n",
    "    mean_charges = ('charges', np.mean)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_with_dummies.corr()\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  Thoughts after performing EDA <h2>\n",
    " Our dataset is from Kaggle so as expected it is really clean and 'pretty'. <br> \n",
    "    \n",
    "   1. There are many more younger patients in our dataset <br>\n",
    "\n",
    "   2. We can observe a linear relationship between age and [mean bmi, mean charges]. Older people probably have more advanced illnesses and that's why the treatment is more expensive <br>\n",
    "\n",
    "   3. There are no serious outliers in our dataset, I think that we can call more extreme observations from our dataset \"natural outliers\", because in real life we also observe situations where a single person spends a lot more on treatment or <br>\n",
    "    or has larger BMI. There is no human error in the dataset. <br>\n",
    "\n",
    "   4. One age group (age = 43) stands out when it comes to the median of charges. This age group also has the largest proportion of smokers <br>\n",
    "    \n",
    "   5. When we analyze the information grouped by region, we can see slight differences (people from northeast pay more, people from southeast have bigger BMI) <br>\n",
    "\n",
    "   6. On average, men pay more for treatment\n",
    "\n",
    "   7. There is a very strong correlation between being a smoker and charges\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Modeling data <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am not splititng the data on train-validation-test because later I will perform GridSearchCV to choose the best version of the model, right now I want to run basic configurations and see first results\n",
    "X = df_with_dummies.loc[:, df_with_dummies.columns != 'charges']\n",
    "y = df_with_dummies['charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base model\n",
    "trees_grid = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 140, 160, 180, 200]\n",
    "results_df = train_test_errors(X, y, trees_grid, RandomForestRegressor() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = results_df.melt(id_vars = 'trees', value_vars = ['train_absolute_error', 'test_absolute_error'])\n",
    "sns.lineplot(data = plot_df, x = 'trees', y = 'value', hue = 'variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hyperparameters tuning <h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting our data to have 10% of data left for the final test of models\n",
    "X_90, X_10, y_90, y_10 = train_test_split(X, y, shuffle=False, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(20, 400, num = 11)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best estimator : RandomForestRegressor(bootstrap=False, max_depth=20, max_features='sqrt',n_estimators=210)\n",
    "results_df, best_estimator = perform_gridsearch(RandomForestRegressor(), param_grid, X_90, y_90, 'neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look more into details of our gridsearch\n",
    "pd.set_option('display.max_columns', None)\n",
    "results = pd.read_csv('data/hyperparameters_tuning_rf.csv', index_col = [0])\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['param_n_estimators'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in results.columns:\n",
    "    if results[col].dtype == 'int' or results[col].dtype == 'float64':\n",
    "        results[col] = abs(results[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_search_results(results, 'param_min_samples_leaf', 'param_n_estimators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_search_results(results, 'param_max_depth', 'param_n_estimators')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing grid search, we may see expected observations on random forest performance:\n",
    "1. We don't observe a situation of overfitting when the number of estimators is higher, but for example when n_estimators = 80, too much depth of a single tree leads to overfitting (test error starts to increase, where train error decreases)\n",
    "2. Increasing max_depth of a tree significantly reduces the error, especially at the first steps (for example increasing the depth from 10 to 20)\n",
    "3. Increasing min_samples_leaf and min_samples_split increases the error, because the tree has less opportunity to split and that's why it does not fit well to certain cases\n",
    "4. Increasing the number of estimators on average helps the algorithm to make better decisions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f640a9078f8e092032cb0185e0c2b2c1ad2376cf3b94da3c4476fa7bf4b3609c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
